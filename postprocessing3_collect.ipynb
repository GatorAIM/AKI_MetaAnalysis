{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618a70f-a558-41f6-a0fc-669b07eab6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import plot_utils\n",
    "import utils_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5262078-50f3-44d0-8e18-c20181e685ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_collectSHAPraw_cross_sub(configs_variables):   \n",
    "    \n",
    "    if not configs_variables[0]['rerun_flag'] and os.path.exists(datafolder+'/shapalltmp.parquet'):\n",
    "        print('Existed: shapalltmp.parquet')\n",
    "        return\n",
    "\n",
    "    shap_data_raws = list()\n",
    "    for configs_variable_m in configs_variables:\n",
    "         for configs_variable_d in configs_variables:\n",
    "            datafolder = configs_variable_m['datafolder']\n",
    "            stg = configs_variable_m['stg']\n",
    "            fs = configs_variable_m['fs']\n",
    "            oversample = configs_variable_m['oversample']\n",
    "            model_type = configs_variable_m['model_type']   \n",
    "\n",
    "            drop_correlation_catboost = configs_variable_m['drop_correlation_catboost']\n",
    "            if drop_correlation_catboost:\n",
    "                suffix = 'nc'\n",
    "            else:\n",
    "                suffix = ''            \n",
    "            tmpdf = pd.read_parquet(datafolder+site_m+'/shapdataraw1d_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet')\n",
    "            tmpdf['site_m'] = 'site_m'\n",
    "            tmpdf['site_d'] = 'site_d'\n",
    "            shap_data_raws.append(tmpdf)\n",
    "\n",
    "    shap_data_raws = pd.concat(shap_data_raws)\n",
    "    shap_data_raws.to_parquet(datafolder+'/shapdataraw1d.parquet')            \n",
    "    \n",
    "    shap_data_raws = list()\n",
    "    for configs_variable_m in configs_variables:\n",
    "         for configs_variable_d in configs_variables:\n",
    "            datafolder = configs_variable_m['datafolder']\n",
    "            stg = configs_variable_m['stg']\n",
    "            fs = configs_variable_m['fs']\n",
    "            oversample = configs_variable_m['oversample']\n",
    "            model_type = configs_variable_m['model_type']   \n",
    "\n",
    "            drop_correlation_catboost = configs_variable_m['drop_correlation_catboost']\n",
    "            if drop_correlation_catboost:\n",
    "                suffix = 'nc'\n",
    "            else:\n",
    "                suffix = ''            \n",
    "            tmpdf = pd.read_parquet(datafolder+site_m+'/shapdataraw2d_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet')\n",
    "            tmpdf['site_m'] = 'site_m'\n",
    "            tmpdf['site_d'] = 'site_d'\n",
    "            shap_data_raws.append(tmpdf)\n",
    "\n",
    "    shap_data_raws = pd.concat(shap_data_raws)\n",
    "    shap_data_raws.to_parquet(datafolder+'/shapdataraw2d.parquet')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9e4b3-e410-4659-81fe-f2fb092a9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_collectSHAPraw_cross_sub_pre(configs_variables, top0=30):\n",
    "    \n",
    "    # get top features\n",
    "    df_importances, df_importances_stat = plot_utils.get_importances_features_stat(configs_variables)\n",
    "    df = df_importances.sort_values('rank', ascending=False).reset_index().groupby('site').head(top0)\n",
    "    top3030 = df[['site', 'Feature Id']].groupby('Feature Id').count().sort_values('site',ascending=False).head(top0)  \n",
    "    top3030 =top3030.index\n",
    "\n",
    "    shap_finals = list()\n",
    "\n",
    "    for configs_variable_m in configs_variables:\n",
    "        for configs_variable_d in configs_variables:\n",
    "            # read datas\n",
    "            year=3000\n",
    "            site_m, datafolder, home_directory = utils_function.get_commons(configs_variable_m)\n",
    "            site_d, datafolder, home_directory = utils_function.get_commons(configs_variable_d)\n",
    "\n",
    "            datafolder = configs_variable_m['datafolder']\n",
    "            stg = configs_variable_m['stg']\n",
    "            fs = configs_variable_m['fs']\n",
    "            oversample = configs_variable_m['oversample']\n",
    "            model_type = configs_variable_m['model_type']   \n",
    "\n",
    "            drop_correlation_catboost = configs_variable_m['drop_correlation_catboost']\n",
    "            if drop_correlation_catboost:\n",
    "                suffix = 'nc'\n",
    "            else:\n",
    "                suffix = ''     \n",
    "\n",
    "            if not configs_variable_m['rerun_flag'] and os.path.exists(datafolder+'/shapalltmp.parquet'):\n",
    "                print('Existed: shapalltmp.parquet')\n",
    "\n",
    "            print('Running collectSHAPraw_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)\n",
    "            tic = time.perf_counter()     \n",
    "\n",
    "            try:\n",
    "                columc_df = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.pkl')\n",
    "                feature_exists = list(set(columc_df.columns) & set(top3030))\n",
    "                \n",
    "                shapX = pd.read_parquet(datafolder+site_m+'/shapdatarawX_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet', columns=feature_exists)\n",
    "                shap = pd.read_parquet(datafolder+site_m+'/shapdataraw_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet', columns=feature_exists)            \n",
    "                \n",
    "                shapX = shapX[feature_exists]\n",
    "                shap = shap[feature_exists]\n",
    "\n",
    "#                 # Reset index to convert the index to a column\n",
    "#                 shapX_reset = shapX.reset_index()\n",
    "#                 shapX_long = pd.melt(shapX_reset, id_vars=['index'], var_name='feature', value_name='value')\n",
    "#                 shapX_long = shapX_long.rename(columns={'index': 'ID'})\n",
    "#                 shapX_long.columns = ['ID', 'feature', 'Name']\n",
    "\n",
    "#                 # Reset index to convert the index to a column\n",
    "#                 shap_reset = shap.reset_index()\n",
    "#                 shap_long = pd.melt(shap_reset, id_vars=['index'], var_name='feature', value_name='value')\n",
    "#                 shap_long = shap_long.rename(columns={'index': 'ID'})\n",
    "\n",
    "#                 shap_final = shap_long.merge(shapX_long, on = ['ID', 'feature'], how='inner')\n",
    "                \n",
    "                shapX.columns = shapX.columns+'_Names'\n",
    "                shap.columns = shap.columns+'_vals'\n",
    "                shap_final = pd.concat([shapX, shap],axis=1)    \n",
    "    \n",
    "                shap_final['site_m'] = site_m\n",
    "                shap_final['site_d'] = site_d\n",
    "\n",
    "                shap_finals.append(shap_final)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    shap_finalX = pd.concat(shap_finals)\n",
    "    shap_finalX.to_parquet(datafolder+'/shapalltmp.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_CDM_PY",
   "language": "python",
   "name": "aki_cdm_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
